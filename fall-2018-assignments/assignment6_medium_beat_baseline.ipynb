{"cells":[{"metadata":{"_uuid":"8a3df70eeceba12519e67c818e99fc08cd0eca71"},"cell_type":"markdown","source":"# Let's find how good is your medium article!"},{"metadata":{"_uuid":"89fd76c52b02a5df9f2bce70ffb6f32ddc1f8bfc"},"cell_type":"markdown","source":"I've done this kernel as a part of the 6th assignment of [mlcourse.ai](http://mlcourse.ai). <br>\nIn this particular kernel i use no hacks (the one with the all 0 submission) and got the MAE ~ 1.76 on the leader board. <br>\nThe very same submission but with the all 0's hack gives around 1.46."},{"metadata":{"trusted":true,"_uuid":"9996f7ec964df7ca4923847151b6a8f8e39cfc56","_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"markdown","source":"Importing all the necessary modules: "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import word_tokenize\nfrom nltk import (PorterStemmer, WordNetLemmatizer)\nfrom sklearn.feature_extraction.text import (CountVectorizer,\n                                             TfidfVectorizer)\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import (RidgeCV, Ridge)\nfrom sklearn.model_selection import (GridSearchCV, \n                                     cross_val_score,\n                                     train_test_split)\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.sparse import (csr_matrix, hstack)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d57fbe6a70896261b8cb90a7fb973140d76bb15"},"cell_type":"markdown","source":"Just a bunch of constantes for easier access to files in kernel:"},{"metadata":{"trusted":true,"_uuid":"e165fcec1e1587fdbbeb4e829e37b9fe7655fbf1"},"cell_type":"code","source":"PATH_TO_DATA = '../input' # modify this if you need to\nTARGET_FILE = 'train_log1p_recommends.csv'\nTARGET_PATH = os.path.join(PATH_TO_DATA, TARGET_FILE)\nTRAIN_FILE = 'train.json'\nTRAIN_PATH = os.path.join(PATH_TO_DATA, TRAIN_FILE)\nTEST_FILE = 'test.json'\nTEST_PATH = os.path.join(PATH_TO_DATA, TEST_FILE)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"749ad8a3f74ead522dfdae26dd382cbe5a216673"},"cell_type":"markdown","source":"Some functions and parts of code are actually reused from [this baseline](http://www.kaggle.com/kashnitsky/ridge-countvectorizer-baseline)"},{"metadata":{"_uuid":"103b4dbfebb063e36a8439fd7650d650e9d88c87"},"cell_type":"markdown","source":"The following code will help to throw away all HTML tags from an article content."},{"metadata":{"trusted":true,"_uuid":"8b55eb273db9e5dda2d08cb976b4b67a3ad360b0"},"cell_type":"code","source":"from html.parser import HTMLParser\n\nclass MLStripper(HTMLParser):\n    def __init__(self):\n        self.reset()\n        self.strict = False\n        self.convert_charrefs = True\n        self.fed = []\n        \n    def handle_data(self, d):\n        self.fed.append(d)\n        \n    def get_data(self):\n        return ''.join(self.fed)\n\ndef strip_tags(html):\n    s = MLStripper()\n    s.feed(html)\n    return s.get_data()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecd78f2cad8c4f1a35e08fb98948e6ec5f193757"},"cell_type":"markdown","source":"Supplementary function to read a JSON line without crashing on escape characters."},{"metadata":{"trusted":true,"_uuid":"68732c8aba79d4d3314c3fcd9259203864cb6a84"},"cell_type":"code","source":"def read_json_line(line=None):\n    result = None\n    try:        \n        result = json.loads(line)\n    except Exception as e:      \n        # Find the offending character index:\n        idx_to_replace = int(str(e).split(' ')[-1].replace(')',''))      \n        # Remove the offending character:\n        new_line = list(line)\n        new_line[idx_to_replace] = ' '\n        new_line = ''.join(new_line)     \n        return read_json_line(line=new_line)\n    return result","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"779e7389e2e50c6d6213193b336deebce8e1956b"},"cell_type":"markdown","source":"And a bunch of functions for feature creation and extraction from .json files"},{"metadata":{"trusted":true,"_uuid":"8e2aa4eb5ec10e33348100448a16794763d12f91"},"cell_type":"code","source":"def get_authors(path_to_file):\n    authors = list()\n    with open(path_to_file, encoding='utf-8') as inp_json_file:\n        for line in inp_json_file:\n            json_data = read_json_line(line)\n            authors.append(json_data['author']['url'].split('@')[1])\n    return authors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b260fcd62171607205cca6ce7ba6a129fe2040dd"},"cell_type":"code","source":"def extract_published_date(path_to_file):\n    dates = list()\n    with open(path_to_file, encoding='utf-8') as inp_json_file:\n        for line in inp_json_file:\n            json_data = read_json_line(line)\n            dates.append(json_data['published']['$date'])\n    dates_df = pd.DataFrame(dates, columns=['date'])\n    dates_df['date'] = pd.to_datetime(dates_df['date'])\n    return dates_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a785e0b8267471d083950dcf0e9dcd9fb51c6c2"},"cell_type":"code","source":"def add_time_features(df, X_sparse):\n    hour = df['date'].apply(lambda ts: ts.hour)\n    morning = ((hour >= 7) & (hour <= 11)).astype('int')\n    day = ((hour >= 12) & (hour <= 18)).astype('int')\n    evening = ((hour >= 19) & (hour <= 23)).astype('int')\n    night = ((hour >= 0) & (hour <= 6)).astype('int')\n    \n    weekday = df['date'].apply(lambda ts: ts.weekday())\n    is_monday = (weekday == 0).astype('int')\n    is_tuesday = (weekday == 1).astype('int')\n    is_wednesday = (weekday == 2).astype('int')\n    is_thursday = (weekday == 3).astype('int')\n    is_friday = (weekday == 4).astype('int')\n    is_weekend = (weekday >= 5).astype('int')\n    \n    X = hstack([X_sparse,\n                morning.values.reshape(-1, 1), \n                day.values.reshape(-1, 1),\n                evening.values.reshape(-1, 1), \n                night.values.reshape(-1, 1),\n                is_monday.values.reshape(-1, 1),\n                is_tuesday.values.reshape(-1, 1),\n                is_wednesday.values.reshape(-1, 1),\n                is_thursday.values.reshape(-1, 1),\n                is_friday.values.reshape(-1, 1),\n                is_weekend.values.reshape(-1, 1)]).tocsr()\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ac0d1bc8994ae97ddc6ea386714aab4d56ee443"},"cell_type":"code","source":"def get_contents(path_to_file):\n    contents = list()\n    with open(path_to_file, encoding='utf-8') as inp_json_file:\n        for line in inp_json_file:\n            json_data = read_json_line(line)\n            content = json_data['content']  \n            contents.append(content)\n    return contents","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1a638bb2f2f800b265066795f77f140d764e70e"},"cell_type":"code","source":"def get_content_features(contents):\n    content_lengths = list()\n    h1_counts = list()\n    h2_counts = list()\n    h3_counts = list()\n    img_counts = list()\n    href_counts = list()\n    \n    for content in contents:\n        content_stripped = strip_tags(content)   \n        content_length = len(content_stripped.split())\n        content_lengths.append(content_length)\n        h1_counts.append(content.count('<h1'))\n        h2_counts.append(content.count('<h2'))\n        h3_counts.append(content.count('<h3'))\n        img_counts.append(content.count('<img'))\n        href_counts.append(content.count('<href'))\n        \n    counts = np.hstack([np.array(h1_counts).reshape(-1, 1),\n                    np.array(h2_counts).reshape(-1, 1),\n                    np.array(h3_counts).reshape(-1, 1),\n                    np.array(img_counts).reshape(-1, 1),\n                    np.array(href_counts).reshape(-1, 1)])\n    \n    content_lengths = np.array(content_lengths)\n    is_short = (content_lengths<1350).astype('int')\n    is_medium = ((content_lengths>=1350) & (content_lengths<2700)).astype('int')\n    is_long = ((content_lengths>=2700) & (content_lengths<6750)).astype('int')\n    is_huge = (content_lengths>=6750).astype('int')\n    \n    length_types = np.hstack([is_short.reshape(-1, 1),\n                              is_medium.reshape(-1, 1),\n                              is_long.reshape(-1, 1),\n                              is_huge.reshape(-1, 1) ])\n    \n    return counts, length_types","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d81e3247064f3ed3371a9572e14e8866aafe77b"},"cell_type":"code","source":"def get_titles(path_to_file):\n    titles = list()\n    with open(path_to_file, encoding='utf-8') as inp_json_file:\n        for line in inp_json_file:\n            json_data = read_json_line(line)\n            title = json_data['title']\n            titles.append(title)\n    return titles","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a448c381244a0876616f2db4e70a6820e4b8aa22"},"cell_type":"code","source":"def get_title_features(titles):\n    titles_lengths = np.array([len(title.split()) for title in titles])\n    is_short = (titles_lengths<6).astype('int')\n    is_medium = ((titles_lengths>=6) & (titles_lengths<11)).astype('int')\n    is_long = ((titles_lengths>=11) & (titles_lengths<20)).astype('int')\n    is_huge = (titles_lengths>=20).astype('int')\n    \n    length_types = np.hstack([is_short.reshape(-1, 1),\n                              is_medium.reshape(-1, 1),\n                              is_long.reshape(-1, 1),\n                              is_huge.reshape(-1, 1) ])\n    return length_types","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cb5607c03200700681839a7a6136cc4f06bd924"},"cell_type":"code","source":"class StemmingLemmatizingTokenizer(object):\n    \n    def __init__(self, stemmer=PorterStemmer, lemmatizer=WordNetLemmatizer):\n        self.stemmer = stemmer()\n        self.lemmatizer = lemmatizer()\n        \n    def __call__(self, doc):\n        # strings of punctuation signs and digits\n        from string import punctuation, digits\n        # some other unicode chars i found in the content\n        other_unicode_chars = '’’”“\\u200b'\n        chars_to_remove = ''.join((punctuation,\n                                   digits,\n                                   other_unicode_chars))\n        # getting rid of punctuation signs and digits\n        transtab = str.maketrans(chars_to_remove, ' '*len(chars_to_remove))\n        # goiinf through all tokens with 3 or more chars\n        # lemmatizing the verbs first, then stemming all words\n        return [self.stemmer.stem(self.lemmatizer.lemmatize(token, pos='v')) \n                for token in word_tokenize(doc.translate(transtab)) \n                if len(token) >= 3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de2856bc2d5f1d61b312468a63ddd43ee3666dff"},"cell_type":"code","source":"from sklearn.feature_extraction import text \nstop_words = text.ENGLISH_STOP_WORDS\ntemp = []\ns = StemmingLemmatizingTokenizer()\nfor eggs in stop_words:\n    token = s(eggs)\n    if token:\n        temp += token\nstop_words = temp","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1b7083c1e8ab00bbbfe7d8c57922428cfd16813"},"cell_type":"markdown","source":"Initializing the vectorizers and scalers since we want our features from test set be scaled and vectorized according to train set."},{"metadata":{"trusted":true,"_uuid":"4a1f8d723ecf2d64d8c21981f1c342bcc99b8aea"},"cell_type":"code","source":"author_vectorizer = CountVectorizer()\ncounts_scaler = StandardScaler()\ncontent_vectorizer = TfidfVectorizer(ngram_range=(1, 2),\n                                     tokenizer=StemmingLemmatizingTokenizer(),\n                                     stop_words=stop_words,\n                                     max_features=200000)\ntitle_vectorizer = TfidfVectorizer(ngram_range=(2, 3), \n                                   tokenizer=StemmingLemmatizingTokenizer(),\n                                   stop_words=stop_words,\n                                   max_features=200000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0241c669206925765834525bd7822568e5677caa"},"cell_type":"markdown","source":"And let's create the train data set!"},{"metadata":{"trusted":true,"_uuid":"5f3f813ebf668e1857fd91883c653b024ded0aaf"},"cell_type":"code","source":"%%time\nauthors = get_authors(TRAIN_PATH)\nauthor_sparse = author_vectorizer.fit_transform(authors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f14593512c7df1f4486ffb17daaba6a61270f29"},"cell_type":"code","source":"%%time\ndate_df = extract_published_date(TRAIN_PATH)\ntrain_data = add_time_features(date_df, author_sparse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"897ef79d00483a0898c9184afa3bf01e2b9b448f"},"cell_type":"code","source":"%%time\nraw_contents = get_contents(TRAIN_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21dd1b0f7896db17debf951d5c037b823ec5f7f1"},"cell_type":"code","source":"%%time\ncounts, length_types = get_content_features(raw_contents)\ncounts_scaled = counts_scaler.fit_transform(counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c3e46b06ec9c598fc91244b4f665abbdd850f36"},"cell_type":"code","source":"%%time\ncontent_sparse = content_vectorizer.fit_transform((strip_tags(content) \n                                                   for content in raw_contents))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f695615d6c0a394037cd9228711cfb8ba9cab23"},"cell_type":"code","source":"%%time\ntitles = get_titles(TRAIN_PATH)\ntitle_length_types = get_title_features(titles)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6b7d05b4cb2e265c317b2992d6352f7560643cf"},"cell_type":"code","source":"%%time\ntitle_sparse = title_vectorizer.fit_transform(titles)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d75287c3cc2c62eebf5fc80074ec82cdd48da1e"},"cell_type":"code","source":"train_data = hstack([train_data,\n                     title_sparse,\n                     title_length_types,\n                     content_sparse,\n                     counts_scaled,\n                     length_types]).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41d0d39b668e8d847be9009f4c68bae42a473cf9"},"cell_type":"markdown","source":"Extracting the target - which is the log(number of recommends)"},{"metadata":{"trusted":true,"_uuid":"d59820cd623b4f8f10c5b0ca9a8bb8ea6cd056a8"},"cell_type":"code","source":"train_target = pd.read_csv(TARGET_PATH, index_col='id')\ntrain_target = train_target['log_recommends'].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aabdfa8108435aa06a3233d8ad8925539fae76a4"},"cell_type":"markdown","source":"Just checking how good the basic Ridge model doest on the train set."},{"metadata":{"trusted":true,"_uuid":"bac8a0a312803188b0300250a509f6518badff52"},"cell_type":"code","source":"%%time\nridge = Ridge()\nX_train, X_test, y_train, y_test = train_test_split(train_data, train_target, random_state=17)\nridge.fit(X_train, y_train);\nridge_pred = ridge.predict(X_test)\nplt.hist(y_test, bins=30, alpha=.5, color='red',\n         label='true values', range=(0,10));\nplt.hist(ridge_pred, bins=30, alpha=.5, color='green',\n         label='predicted values', range=(0,10));\nplt.legend();\nvalid_mae = mean_absolute_error(y_test, ridge_pred)\nprint(valid_mae, np.expm1(valid_mae))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ac8c89544752e5cfbcf2eb7f476ba6efe415ea8"},"cell_type":"markdown","source":"Now we training our models with the full train data."},{"metadata":{"trusted":true,"_uuid":"645f76b6d77fd34e969c925bf2336cd8055ddcab"},"cell_type":"code","source":"%%time\nalphas = (0.005, 0.01, 0.5, 0.1, 1)\nridge = RidgeCV(alphas=alphas, cv=5, gcv_mode='auto',\n                scoring='neg_mean_absolute_error')\nridge.fit(train_data, train_target);\nprint('alpha: ', ridge.alpha_) # i'm just curious what it would be","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4e9069d60810f8f27b63ef4f29b7508b1e4eaf7"},"cell_type":"markdown","source":"And let's create test data finally."},{"metadata":{"trusted":true,"_uuid":"a83101acfce6f6ea4cffbda9f554e480beb9b007"},"cell_type":"code","source":"%%time\nauthors = get_authors(TEST_PATH)\nauthor_sparse = author_vectorizer.transform(authors)\n\ndate_df = extract_published_date(TEST_PATH)\ntest_data = add_time_features(date_df, author_sparse)\n\nraw_contents = get_contents(TEST_PATH)\ncounts, length_types = get_content_features(raw_contents)\ncounts_scaled = counts_scaler.transform(counts)\n\nstripped_contents = [strip_tags(content) for content in raw_contents]\ncontent_sparse = content_vectorizer.transform(stripped_contents)\n\ntitles = get_titles(TEST_PATH)\ntitle_length_types = get_title_features(titles)\ntitle_sparse = title_vectorizer.transform(titles)\n\ntest_data = hstack([test_data, \n                    title_sparse,\n                    title_length_types,\n                    content_sparse, \n                    counts_scaled,\n                    length_types]).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7056c5670fcc3ccefdc7b96d7383be07b791eea"},"cell_type":"markdown","source":"Predicting, writing to file and checking the public score!"},{"metadata":{"trusted":true,"_uuid":"c5004b89392d2489dc430f9db142e17ab13e55c5"},"cell_type":"code","source":"ridge_test_pred = ridge.predict(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"464efa0a7730061ae3e779e278ba244ce6445587"},"cell_type":"code","source":"def write_submission_file(prediction, filename,\n    path_to_sample=os.path.join(PATH_TO_DATA, 'sample_submission.csv')):\n    submission = pd.read_csv(path_to_sample, index_col='id')\n    \n    submission['log_recommends'] = prediction\n    submission.to_csv(filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77a664b5522dd0b3a8c4054b4ac9fdbb3a5fe8fd"},"cell_type":"code","source":"write_submission_file(prediction=ridge_test_pred, \n                      filename='ridgeCV_200k_stemming_lemmatizing.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4559e16fb3e749e58af23ebe0c0c4ce53055b3e"},"cell_type":"markdown","source":"As i've said earlier we can do a little hack on this assignment. <br>\nIf we submit all zeroes submission we will get the mean log recommends from test set. <br>\nHow to use it it's only yours decision!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}